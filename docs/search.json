[
  {
    "objectID": "Homework9.html",
    "href": "Homework9.html",
    "title": "Homework 9",
    "section": "",
    "text": "Output off.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(tree)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(baguette)\nlibrary(ranger)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      local = locale(encoding = \"latin1\"))\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\nbike_data &lt;- bike_data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\nbike_data &lt;- bike_data |&gt;\n  rename('bike_count' = `Rented Bike Count`,\n         'hour' = \"Hour\",\n         \"temp\" = `Temperature(°C)`,\n         \"wind_speed\" = `Wind speed (m/s)`,\n         \"humidity\" = `Humidity(%)`,\n         \"vis\" = `Visibility (10m)`,\n         \"dew_point_temp\" = `Dew point temperature(°C)`,\n         \"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n         \"rainfall\" = \"Rainfall(mm)\",\n         \"snowfall\" = `Snowfall (cm)`)\nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind_speed = mean(wind_speed),\n            vis = mean(vis),\n            dew_point_temp = mean(dew_point_temp),\n            solar_radiation = mean(solar_radiation),\n            rainfall = sum(rainfall),\n            snowfall = sum(snowfall)) |&gt;\n            ungroup()\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\n#Cleaned up code.  We found MLR_rec1 to be the best model in HW8.\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#Need this code snippet for unfitted for later\nMLR_wkf1 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec)\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nfinal_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split,metrics=metric_set(rmse,mae))"
  },
  {
    "objectID": "Homework9.html#previous-code-from-hw8.",
    "href": "Homework9.html#previous-code-from-hw8.",
    "title": "Homework 9",
    "section": "",
    "text": "Output off.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(tree)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(baguette)\nlibrary(ranger)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      local = locale(encoding = \"latin1\"))\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\nbike_data &lt;- bike_data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\nbike_data &lt;- bike_data |&gt;\n  rename('bike_count' = `Rented Bike Count`,\n         'hour' = \"Hour\",\n         \"temp\" = `Temperature(°C)`,\n         \"wind_speed\" = `Wind speed (m/s)`,\n         \"humidity\" = `Humidity(%)`,\n         \"vis\" = `Visibility (10m)`,\n         \"dew_point_temp\" = `Dew point temperature(°C)`,\n         \"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n         \"rainfall\" = \"Rainfall(mm)\",\n         \"snowfall\" = `Snowfall (cm)`)\nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind_speed = mean(wind_speed),\n            vis = mean(vis),\n            dew_point_temp = mean(dew_point_temp),\n            solar_radiation = mean(solar_radiation),\n            rainfall = sum(rainfall),\n            snowfall = sum(snowfall)) |&gt;\n            ungroup()\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\n#Cleaned up code.  We found MLR_rec1 to be the best model in HW8.\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#Need this code snippet for unfitted for later\nMLR_wkf1 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec)\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nfinal_fit &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split,metrics=metric_set(rmse,mae))"
  },
  {
    "objectID": "Homework9.html#lasso-model",
    "href": "Homework9.html#lasso-model",
    "title": "Homework 9",
    "section": "Lasso Model",
    "text": "Lasso Model\nTo Recap:\nOur coefficient table for our MLR model from the previous HW.\n\n#Final Model\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3980. Preprocessor1_Model1\n2 mae     standard       3039. Preprocessor1_Model1\n\nfinal_fit_co &lt;- final_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\nfinal_fit_co\n\n# A tibble: 14 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         17446.       252.    69.3   9.38e-165\n 2 temp                -2439.      5215.    -0.468 6.40e-  1\n 3 humidity            -1927.      1904.    -1.01  3.13e-  1\n 4 wind_speed           -523.       286.    -1.83  6.86e-  2\n 5 vis                   -63.7      361.    -0.177 8.60e-  1\n 6 dew_point_temp       7143.      6143.     1.16  2.46e-  1\n 7 solar_radiation      4088.       473.     8.64  6.74e- 16\n 8 rainfall            -1779.       333.    -5.35  2.00e-  7\n 9 snowfall             -317.       276.    -1.15  2.50e-  1\n10 seasons_Spring      -2528.       355.    -7.12  1.14e- 11\n11 seasons_Summer      -1670.       442.    -3.78  1.98e-  4\n12 seasons_Winter      -3684.       501.    -7.35  2.88e- 12\n13 holiday_No.Holiday    835.       256.     3.26  1.28e-  3\n14 day_type_Weekend    -1050.       256.    -4.10  5.56e-  5\n\n\nNow finding our LASSO model.\n\nset.seed(11)  \nLASSO_spec &lt;-linear_reg(penalty=tune(),mixture=1) |&gt;\n  set_engine(\"glmnet\")\n\nLASSO_wkf &lt;-workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(LASSO_spec)\n\n#Fitting the model\nLASSO_grid &lt;-LASSO_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200)) \nLASSO_grid[1, \".metrics\"][[1]]\n\n[[1]]\n# A tibble: 400 × 5\n    penalty .metric .estimator .estimate .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard       4784. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard       4784. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard       4784. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard       4784. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard       4784. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard       4784. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard       4784. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard       4784. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard       4784. Preprocessor1_Model009\n10 2.83e-10 rmse    standard       4784. Preprocessor1_Model010\n# ℹ 390 more rows\n\n\n200 seperate Lasso models. Plotting our RMSE.\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nInteresting that they are the same. Finding the lowest penalty.\n\nlowest_rmse &lt;-LASSO_grid |&gt;\n  select_best(metric=\"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\nFitting our best model and printing coefficients.\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  fit(bike_train)\n  tidy(LASSO_final)\n\n# A tibble: 14 × 3\n   term               estimate      penalty\n   &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)          17446. 0.0000000001\n 2 temp                   389. 0.0000000001\n 3 humidity              -887. 0.0000000001\n 4 wind_speed            -522. 0.0000000001\n 5 vis                      0  0.0000000001\n 6 dew_point_temp        3752. 0.0000000001\n 7 solar_radiation       4065. 0.0000000001\n 8 rainfall             -1841. 0.0000000001\n 9 snowfall              -336. 0.0000000001\n10 seasons_Spring       -2505. 0.0000000001\n11 seasons_Summer       -1607. 0.0000000001\n12 seasons_Winter       -3653. 0.0000000001\n13 holiday_No.Holiday     820. 0.0000000001\n14 day_type_Weekend     -1060. 0.0000000001"
  },
  {
    "objectID": "Homework9.html#regression-tree-model",
    "href": "Homework9.html#regression-tree-model",
    "title": "Homework 9",
    "section": "Regression Tree Model",
    "text": "Regression Tree Model\nSeeing rough plot of our regression tree.\n\nfitTree &lt;- tree(bike_count ~ ., data = bike_train) \n\nWarning in tree(bike_count ~ ., data = bike_train): NAs introduced by coercion\n\nplot(fitTree)\ntext(fitTree)\n\n\n\n\n\n\n\n\nFitting/Tuning our model.\n\nset.seed(11)\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n                          set_engine(\"rpart\") |&gt;\n                          set_mode(\"regression\")\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(tree_mod)\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid)\ntree_fits |&gt;\n  collect_metrics() \n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6432.       10 350.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.595    10   0.0484 Prepro…\n 3    0.000000001           1 rmse    standard   6432.       10 350.     Prepro…\n 4    0.000000001           1 rsq     standard      0.595    10   0.0484 Prepro…\n 5    0.00000001            1 rmse    standard   6432.       10 350.     Prepro…\n 6    0.00000001            1 rsq     standard      0.595    10   0.0484 Prepro…\n 7    0.0000001             1 rmse    standard   6432.       10 350.     Prepro…\n 8    0.0000001             1 rsq     standard      0.595    10   0.0484 Prepro…\n 9    0.000001              1 rmse    standard   6432.       10 350.     Prepro…\n10    0.000001              1 rsq     standard      0.595    10   0.0484 Prepro…\n# ℹ 90 more rows\n\n\nPlotting our RMSE values by tree depth.\n\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.001                11 rmse    standard   3817.    10    285. Preprocess…\n 2    0.001                15 rmse    standard   3817.    10    285. Preprocess…\n 3    0.0000000001         11 rmse    standard   3837.    10    302. Preprocess…\n 4    0.000000001          11 rmse    standard   3837.    10    302. Preprocess…\n 5    0.00000001           11 rmse    standard   3837.    10    302. Preprocess…\n 6    0.0000001            11 rmse    standard   3837.    10    302. Preprocess…\n 7    0.000001             11 rmse    standard   3837.    10    302. Preprocess…\n 8    0.00001              11 rmse    standard   3837.    10    302. Preprocess…\n 9    0.0001               11 rmse    standard   3837.    10    302. Preprocess…\n10    0.0000000001         15 rmse    standard   3837.    10    302. Preprocess…\n# ℹ 40 more rows\n\n\nLooks like tree depth of 11 or possibly 15 is our best fit. Finding the best model based on rmse.\n\ntree_best_params &lt;- select_best(tree_fits,metric= \"rmse\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\n\nFitting our final model and plotting the tree.\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(bike_split)\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3096.    Preprocessor1_Model1\n2 rsq     standard       0.905 Preprocessor1_Model1\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)"
  },
  {
    "objectID": "Homework9.html#bagged-tree-model",
    "href": "Homework9.html#bagged-tree-model",
    "title": "Homework 9",
    "section": "Bagged Tree Model",
    "text": "Bagged Tree Model\nFitting and bagging our model.\n\nbag_spec &lt;- bag_tree(tree_depth = 15, min_n = 20, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nbag_wkf &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(bag_spec)\n\nbag_fit &lt;- bag_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15),\n            metrics = metric_set(rmse))\nbag_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        3.73e- 8 rmse    standard   3216.    10    182. Preprocessor1_Model05\n 2        4.39e-10 rmse    standard   3256.    10    170. Preprocessor1_Model02\n 3        8.48e- 9 rmse    standard   3273.    10    194. Preprocessor1_Model04\n 4        2.68e- 4 rmse    standard   3274.    10    200. Preprocessor1_Model11\n 5        1.64e- 7 rmse    standard   3283.    10    190. Preprocessor1_Model06\n 6        1.39e- 5 rmse    standard   3287.    10    179. Preprocessor1_Model09\n 7        1.18e- 3 rmse    standard   3312.    10    173. Preprocessor1_Model12\n 8        7.20e- 7 rmse    standard   3323.    10    173. Preprocessor1_Model07\n 9        6.11e- 5 rmse    standard   3330.    10    172. Preprocessor1_Model10\n10        1   e-10 rmse    standard   3362.    10    171. Preprocessor1_Model01\n11        1.93e- 9 rmse    standard   3379.    10    219. Preprocessor1_Model03\n12        3.16e- 6 rmse    standard   3431.    10    180. Preprocessor1_Model08\n13        5.18e- 3 rmse    standard   3532.    10    161. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3956.    10    150. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4982.    10    170. Preprocessor1_Model15\n\n\nSelecting the best model.\n\nbag_best_parmas &lt;- select_best(bag_fit,metric=\"rmse\")\n#Best model\nbag_best_parmas\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1    0.0000000373 Preprocessor1_Model05\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_parmas)\nbag_final_fit &lt;- bag_final_wkf |&gt;\n  last_fit(bike_split)\nbag_final_fit |&gt;\n    collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3084.    Preprocessor1_Model1\n2 rsq     standard       0.907 Preprocessor1_Model1\n\n\nPlotting variables by importance.\n\nbag_imp &lt;- extract_fit_engine(bag_final_fit)\nbag_imp$imp |&gt;\n  mutate(term = factor(term,levels=term)) |&gt;\n  ggplot(aes(x=term,y=value)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variables of Importance\",\n    x = \"Variables\",\n    y = \"Importance\"\n  )"
  },
  {
    "objectID": "Homework9.html#random-forest-model",
    "href": "Homework9.html#random-forest-model",
    "title": "Homework 9",
    "section": "Random Forest Model",
    "text": "Random Forest Model\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  #it took me forever to figure out adding the impurity\n  set_engine(\"ranger\",importance=\"impurity\") |&gt;\n  set_mode(\"regression\")\n\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(rf_spec)\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = 10,\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric ==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n    mtry .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    11 rmse    standard   3013.    10    199. Preprocessor1_Model07\n 2    13 rmse    standard   3014.    10    191. Preprocessor1_Model03\n 3     8 rmse    standard   3030.    10    194. Preprocessor1_Model08\n 4     9 rmse    standard   3037.    10    200. Preprocessor1_Model01\n 5    10 rmse    standard   3044.    10    193. Preprocessor1_Model06\n 6     6 rmse    standard   3080.    10    195. Preprocessor1_Model05\n 7     5 rmse    standard   3102.    10    194. Preprocessor1_Model10\n 8     4 rmse    standard   3156.    10    189. Preprocessor1_Model09\n 9     3 rmse    standard   3212.    10    176. Preprocessor1_Model02\n10     2 rmse    standard   3455.    10    166. Preprocessor1_Model04\n\n\nSelecting best model\n\nrf_best_params &lt;-select_best(rf_fit,metric=\"rmse\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config              \n  &lt;int&gt; &lt;chr&gt;                \n1    11 Preprocessor1_Model07\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(bike_split)\n\nPlotting Variables by importance.\n\nrf_imp &lt;- rf_final_fit |&gt;\n  extract_fit_parsnip()\n\nrf_imp_tib &lt;- enframe(\n  rf_imp$fit$variable.importance,\n  name = \"Variable\",\n  value = \"Importance\"\n) |&gt;\n  arrange(desc(Importance))\n\nggplot(rf_imp_tib, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variables of Importance\",\n    x = \"Variables\",\n    y = \"Importance\"\n  )"
  },
  {
    "objectID": "Homework9.html#comparing-all-models",
    "href": "Homework9.html#comparing-all-models",
    "title": "Homework 9",
    "section": "Comparing all Models",
    "text": "Comparing all Models\nMLR RMSE and MAE:\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3980. Preprocessor1_Model1\n2 mae     standard       3039. Preprocessor1_Model1\n\n\nLasso RMSE and MAE:\n\nlasso_metrics &lt;- LASSO_final |&gt;\n  predict(bike_test) |&gt;\n  bind_cols(bike_test) |&gt;\n  metrics(truth = bike_count, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\"))\nlasso_metrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3999.\n2 mae     standard       3063.\n\n\nRegression Tree RMSE and MAE:\n\ntree_final &lt;- tree_final_wkf |&gt;\n  finalize_workflow(tree_best_params) |&gt;\n  fit(bike_train)\n\ntree_metrics &lt;- tree_final |&gt;\n  predict(bike_test) |&gt;\n  bind_cols(bike_test) |&gt;\n  metrics(truth = bike_count, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\"))\ntree_metrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3096.\n2 mae     standard       2362.\n\n\nBagged Tree RMSE and MAE:\n\nbag_final &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_parmas) |&gt;\n  fit(bike_train)\n\nbag_metrics &lt;- bag_final |&gt;\n  predict(bike_test) |&gt;\n  bind_cols(bike_test) |&gt;\n  metrics(truth = bike_count, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\"))\nbag_metrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3001.\n2 mae     standard       2372.\n\n\nRandom Forest RMSE and MAE:\n\nrf_final &lt;- rf_final_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  fit(bike_train)\n\nrf_metrics &lt;- rf_final |&gt;\n  predict(bike_test) |&gt;\n  bind_cols(bike_test) |&gt;\n  metrics(truth = bike_count, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\"))\ntree_metrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3096.\n2 mae     standard       2362.\n\n\nBased on these values its hard to choose the exact best model. The bagged tree model has the lowest RMSE while random forest/regression tree have the lowest MAEs. Overall it looks like the bagged tree model works best."
  },
  {
    "objectID": "Homework9.html#final-fit",
    "href": "Homework9.html#final-fit",
    "title": "Homework 9",
    "section": "Final Fit",
    "text": "Final Fit\nFitting to entire dataset.\n\nbag_final_fitted &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_parmas) |&gt;\n  fit(bike_data)\nbag_final_fitted\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: bag_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               24435074618. 323686050.    11\n 2 dew_point_temp     20249058377. 373596038.    11\n 3 solar_radiation    16394350753. 308631743.    11\n 4 seasons_Winter     14868447566. 511701608.    11\n 5 humidity           10144347684. 365327408.    11\n 6 rainfall            4526568153. 444264734.    11\n 7 seasons_Summer      3053670335. 980074319.    11\n 8 seasons_Spring      2219357378. 221822818.    11\n 9 snowfall            2002996175. 691068746.    10\n10 wind_speed          1648381587. 340863119.    11\n11 vis                 1316309005. 208399310.    11\n12 day_type_Weekend     228322651.  40254588.    11\n13 holiday_No.Holiday    38907538.  15491733.    11"
  }
]